---
layout: blog_base.njk
title: XXXX
description: asasasasasasas
author: Saeed Mirshekari
date: 2022-05-13T14:21:56.263Z
tags:
  - post
thumbnail: /assets/img/uploads/logos.png
rating: 5
---
# Business Problem

The British Columbia Hospice Palliative Care Association (BCHPCA), one of the Founding Organizations of the Provincial Hospice Work Group (PHWG) would like to conduct business analysis through this case study to determine the trend of government funding over the past 5 years and correlating that with growth in services expenditures.  This study will try to predict future expenditures and determine government funding share to support them.

## Data Description

The dataset was built from scratch and sourced from the following resources :
1.	Canada Revenue Agency 2021
2. 	British Columbia - Population Estimates & Projections 
3.  Statistics Canada for Consumer Price Index and Health Care Price Index Monthly (British Columbia)
4.  WIKIPEDIA List of municipalities in British Columbia

Hospice General Information:

    - Year - fiscal year when CRA report was generated
    - Region - regional health territory where the hospice is registered and operating
    - Type - Residential “R” (hospital based) and Community “C”
    - City - nearest urban area where hospice is operating the pallative services
    - Population - population is based on a per city population sourced from BC Stats. However, due to unavailability of some city data on a per year, some city population were derived from the yearly regional population and further reduced to city-size by basing it on the 2016 city statitics sourced from Wikipedia.

Population derivative formula (for cities with no population data):

    - Total Population = City population 2016 + City population % change from 2016 (based on yearly regional population change)
    - Elderly population from 65+ = population share of 65+ elderly population (based on yearly regional BC Stats) x Total Population

Hospices Revenue includes the following sources:

    - Govt Funding - these are funds sourced from the federal government
    - Donations - these are donated funds to hospices regardless receipted or non-receipted
    - Others - these are gifts from other charities. other revenue from fund raising is included.

Hospices Expenses are broken down as follows:

    - Charitable Expenditures
    - Management & Administration
    - Fund Raising
    - Political Activities
    - Gifts to other registered charities
    - Other

**Note**: Total Expenses includes already Compensation and Professional & Consulting. It is categorized under Management & Administration

Descriptive and numeric analysis of hospices’ revenues and services expenditures was performed in the first part of the case study. In this part we are going to find out the quantitative relationships between government funding and expenditures. This should help to evaluate the approach to government funding and planning. 
Additionally, it was assumed that there are also indicators other than financial that fuel services growth in British Columbia. The domain expert advised that the major factor influencing the expenditures is growth of an aging population (older than 65). Thus, we will also try to predict future expenses based on funding amount and 65+population.


# Import Required Libraries


```python
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
plt.rcParams["figure.figsize"] = [10, 6]
import seaborn as sns
import statsmodels.formula.api as smf
from sklearn.linear_model import LinearRegression,Ridge,Lasso
from sklearn import metrics
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_squared_error
```

# Loading Data


```python
df = pd.read_excel('CRA_DATA.xlsx')
df1 = df.rename(columns={'Hospice Name':'Hospice', 'Others':'Other_rev', 'Other':'Other_ex', 'Govt Funding':'Govt_Funding', 'Charitable Expenditures':'Charitable_Ex', 'Management & Administration':'Management_Admin', 'Fund Raising':'Fund_Raising', 'Political Activities':'Political_Activities', 'Gifts to other registered charities':'Gifts_to_others', 'Professional & Consulting':'Professional_Consulting', 'FULL - TIME STAFF':'Full_Time_Staff', 'PART - TIME STAFF':'Part_time_Staff', 'Type Of Hospice':'Type', 'Population total':'Population_total', '65_over':'Seniors'})
df1.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Hospice</th>
      <th>Year</th>
      <th>Region</th>
      <th>City</th>
      <th>Population_total</th>
      <th>Seniors</th>
      <th>Type</th>
      <th>Donations</th>
      <th>Other_rev</th>
      <th>Govt_Funding</th>
      <th>...</th>
      <th>Charitable_Ex</th>
      <th>Management_Admin</th>
      <th>Fund_Raising</th>
      <th>Political_Activities</th>
      <th>Gifts_to_others</th>
      <th>Other_ex</th>
      <th>Compensation</th>
      <th>Professional_Consulting</th>
      <th>Full_Time_Staff</th>
      <th>Part_time_Staff</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>100 Mile District Hospice Palliative Care Society</td>
      <td>2021</td>
      <td>Interior Health</td>
      <td>100 MILE HOUSE</td>
      <td>15109.0</td>
      <td>4509.0</td>
      <td>C</td>
      <td>116674</td>
      <td>73450</td>
      <td>43786</td>
      <td>...</td>
      <td>119077</td>
      <td>7020</td>
      <td>15866</td>
      <td>0.0</td>
      <td>47176.0</td>
      <td>0.0</td>
      <td>56135</td>
      <td>1577</td>
      <td>0</td>
      <td>3</td>
    </tr>
    <tr>
      <th>1</th>
      <td>100 Mile District Hospice Palliative Care Society</td>
      <td>2020</td>
      <td>Interior Health</td>
      <td>100 MILE HOUSE</td>
      <td>15119.0</td>
      <td>4260.0</td>
      <td>C</td>
      <td>38222</td>
      <td>1352</td>
      <td>38487</td>
      <td>...</td>
      <td>58918</td>
      <td>17373</td>
      <td>0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>46649</td>
      <td>1538</td>
      <td>0</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>100 Mile District Hospice Palliative Care Society</td>
      <td>2019</td>
      <td>Interior Health</td>
      <td>100 MILE HOUSE</td>
      <td>14971.0</td>
      <td>3998.0</td>
      <td>C</td>
      <td>28088</td>
      <td>16939</td>
      <td>33322</td>
      <td>...</td>
      <td>52266</td>
      <td>9473</td>
      <td>0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>45288</td>
      <td>1500</td>
      <td>0</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>100 Mile District Hospice Palliative Care Society</td>
      <td>2018</td>
      <td>Interior Health</td>
      <td>100 MILE HOUSE</td>
      <td>14911.0</td>
      <td>3901.0</td>
      <td>C</td>
      <td>24326</td>
      <td>5664</td>
      <td>32706</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0.0</td>
      <td>58263.0</td>
      <td>0.0</td>
      <td>40730</td>
      <td>1587</td>
      <td>0</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>100 Mile District Hospice Palliative Care Society</td>
      <td>2017</td>
      <td>Interior Health</td>
      <td>100 MILE HOUSE</td>
      <td>14932.0</td>
      <td>3801.0</td>
      <td>C</td>
      <td>3850</td>
      <td>2010</td>
      <td>0</td>
      <td>...</td>
      <td>5367</td>
      <td>0</td>
      <td>0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>4618</td>
      <td>0</td>
      <td>0</td>
      <td>3</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 21 columns</p>
</div>



# Filtering Data


```python
# creating a dataset with only community hospices as their funding may be diffrent from residential hospital based ones.
df_com = df1[df1['Type'] == 'C']
df_com.info()
```

    <class 'pandas.core.frame.DataFrame'>
    Int64Index: 302 entries, 0 to 346
    Data columns (total 21 columns):
     #   Column                   Non-Null Count  Dtype         
    ---  ------                   --------------  -----         
     0   Hospice                  302 non-null    object        
     1   Year                     302 non-null    int64         
     2   Region                   302 non-null    object        
     3   City                     302 non-null    object        
     4   Population_total         302 non-null    float64       
     5   Seniors                  302 non-null    float64       
     6   Type                     302 non-null    object        
     7   Donations                302 non-null    int64         
     8   Other_rev                302 non-null    int64         
     9   Govt_Funding             302 non-null    int64         
     10  Date                     302 non-null    datetime64[ns]
     11  Charitable_Ex            302 non-null    int64         
     12  Management_Admin         302 non-null    int64         
     13  Fund_Raising             302 non-null    int64         
     14  Political_Activities     283 non-null    float64       
     15  Gifts_to_others          283 non-null    float64       
     16  Other_ex                 300 non-null    float64       
     17  Compensation             302 non-null    int64         
     18  Professional_Consulting  302 non-null    int64         
     19  Full_Time_Staff          302 non-null    int64         
     20  Part_time_Staff          302 non-null    int64         
    dtypes: datetime64[ns](1), float64(5), int64(11), object(4)
    memory usage: 51.9+ KB
    

# Exploring Data


```python
# It is assumed that Gov_Funding and Seniors are independent variables and both influence Charitable_Ex (target var).
# The correlation matrix (Pearson’s correlation) of the independent variables helps to detect if collinearity exists.
# First, for the regression model with all observations included (both hospices types).
a4_dims = (15, 10)
fig, ax = plt.subplots(figsize=a4_dims)
sns.heatmap(df1.corr(), annot = True, vmin=-1, vmax=1, center= 0, cmap= 'coolwarm', linewidths=3, linecolor='black',ax=ax)
```




    <matplotlib.axes._subplots.AxesSubplot at 0x1e40155f2c8>




![png](output_9_1.png)


## Conclusions:
We don’t have a severe collinearity among Gov_Funding and Seniors (0.52).
Gov_Funding and Donations are highly correlated with each other (0.9). Thus, they are collinear, and we continue only with Gov_Funding.
Population and Seniors have moderately high collinearity (0.84). Although they measure different things, higher population generally corresponds to higher seniors, so we continue only with Seniors.


```python
# Second, for the regression model with community type hospices observations included.
a4_dims = (15, 10)
fig, ax = plt.subplots(figsize=a4_dims)
sns.heatmap(df_com.corr(), annot = True, vmin=-1, vmax=1, center= 0, cmap= 'coolwarm', linewidths=3, linecolor='black',ax=ax)
```




    <matplotlib.axes._subplots.AxesSubplot at 0x1e401535608>




![png](output_11_1.png)


## Conclusions:
We don’t have a severe collinearity among Gov_Funding and Seniors (0.57).
Population and Seniors have moderately high collinearity (0.84).
This time, Gov_Funding and Donations are not collinear (0.55), and both can be included in the regression model. 

# Correlation Plots


```python
# Check correlation between Seniors and Charitable_Ex depending on Hospice Type
ax = sns.lmplot(x="Seniors", y="Charitable_Ex", hue="Type", data=df1,
           markers=["o", "x"], palette="Set1")
```


![png](output_14_0.png)



```python
# Check correlation between Gov_Funding and Charitable_Ex depending on Hospice Type
ax = sns.lmplot(x="Govt_Funding", y="Charitable_Ex", hue="Type", data=df1,
           markers=["o", "x"], palette="Set1")
```


![png](output_15_0.png)



```python
# Same as above but in log-scale axis
# Check correlation between Gov_Funding and Charitable_Ex depending on Hospice Type
ax = sns.lmplot(x="Govt_Funding", y="Charitable_Ex", hue="Type", data=df1,
           markers=["o", "x"], palette="Set1")
ax.set(xscale="log", yscale="log")
```




    <seaborn.axisgrid.FacetGrid at 0x1e47f006bc8>




![png](output_16_1.png)


## Conclusion:
Govt_Funding shows linear correlation with Charitable_Ex for both hospice types.
Seniors show weaker linear correlation with Charitable_Ex for Residential hospice type, and may affect regression results. 
To check this assumption we may test regression models with all observations and community observations separately.

# Multiple Regression Models and Evaluation (dataset with all observations)


```python
df1.columns
```




    Index(['Hospice', 'Year', 'Region', 'City', 'Population_total', 'Seniors',
           'Type', 'Donations', 'Other_rev', 'Govt_Funding', 'Date',
           'Charitable_Ex', 'Management_Admin', 'Fund_Raising',
           'Political_Activities', 'Gifts_to_others', 'Other_ex', 'Compensation',
           'Professional_Consulting', 'Full_Time_Staff', 'Part_time_Staff'],
          dtype='object')




```python
df1.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Hospice</th>
      <th>Year</th>
      <th>Region</th>
      <th>City</th>
      <th>Population_total</th>
      <th>Seniors</th>
      <th>Type</th>
      <th>Donations</th>
      <th>Other_rev</th>
      <th>Govt_Funding</th>
      <th>...</th>
      <th>Charitable_Ex</th>
      <th>Management_Admin</th>
      <th>Fund_Raising</th>
      <th>Political_Activities</th>
      <th>Gifts_to_others</th>
      <th>Other_ex</th>
      <th>Compensation</th>
      <th>Professional_Consulting</th>
      <th>Full_Time_Staff</th>
      <th>Part_time_Staff</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>100 Mile District Hospice Palliative Care Society</td>
      <td>2021</td>
      <td>Interior Health</td>
      <td>100 MILE HOUSE</td>
      <td>15109.0</td>
      <td>4509.0</td>
      <td>C</td>
      <td>116674</td>
      <td>73450</td>
      <td>43786</td>
      <td>...</td>
      <td>119077</td>
      <td>7020</td>
      <td>15866</td>
      <td>0.0</td>
      <td>47176.0</td>
      <td>0.0</td>
      <td>56135</td>
      <td>1577</td>
      <td>0</td>
      <td>3</td>
    </tr>
    <tr>
      <th>1</th>
      <td>100 Mile District Hospice Palliative Care Society</td>
      <td>2020</td>
      <td>Interior Health</td>
      <td>100 MILE HOUSE</td>
      <td>15119.0</td>
      <td>4260.0</td>
      <td>C</td>
      <td>38222</td>
      <td>1352</td>
      <td>38487</td>
      <td>...</td>
      <td>58918</td>
      <td>17373</td>
      <td>0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>46649</td>
      <td>1538</td>
      <td>0</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>100 Mile District Hospice Palliative Care Society</td>
      <td>2019</td>
      <td>Interior Health</td>
      <td>100 MILE HOUSE</td>
      <td>14971.0</td>
      <td>3998.0</td>
      <td>C</td>
      <td>28088</td>
      <td>16939</td>
      <td>33322</td>
      <td>...</td>
      <td>52266</td>
      <td>9473</td>
      <td>0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>45288</td>
      <td>1500</td>
      <td>0</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>100 Mile District Hospice Palliative Care Society</td>
      <td>2018</td>
      <td>Interior Health</td>
      <td>100 MILE HOUSE</td>
      <td>14911.0</td>
      <td>3901.0</td>
      <td>C</td>
      <td>24326</td>
      <td>5664</td>
      <td>32706</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0.0</td>
      <td>58263.0</td>
      <td>0.0</td>
      <td>40730</td>
      <td>1587</td>
      <td>0</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>100 Mile District Hospice Palliative Care Society</td>
      <td>2017</td>
      <td>Interior Health</td>
      <td>100 MILE HOUSE</td>
      <td>14932.0</td>
      <td>3801.0</td>
      <td>C</td>
      <td>3850</td>
      <td>2010</td>
      <td>0</td>
      <td>...</td>
      <td>5367</td>
      <td>0</td>
      <td>0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>4618</td>
      <td>0</td>
      <td>0</td>
      <td>3</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 21 columns</p>
</div>




```python
### STATSMODELS ###

# include Gov_Funding and Seniors in the model
lm_mult = smf.ols(formula='Charitable_Ex ~ Govt_Funding ', data=df1).fit()
lm_mult.rsquared
```




    0.8881987555698252




```python
lm_mult.pvalues
```




    Intercept        5.902203e-04
    Govt_Funding    3.276033e-166
    dtype: float64




```python
lm_mult.params
```




    Intercept       90119.587176
    Govt_Funding        1.210820
    dtype: float64




```python
lm_mult.conf_int()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Intercept</th>
      <td>39011.902619</td>
      <td>141227.271733</td>
    </tr>
    <tr>
      <th>Govt_Funding</th>
      <td>1.165331</td>
      <td>1.256310</td>
    </tr>
  </tbody>
</table>
</div>



## Conclusion:
R-squared is 0.89
P-values for Govt_Funding and Seniors coefficients are less than 0.05
Stat regression model shows reliable evaluation indicators, so we can calculate regression coefficients.

# Modeling: Linear Regression


```python
sns.distplot(df1['Charitable_Ex'])
```




    <matplotlib.axes._subplots.AxesSubplot at 0x1e40206d588>




![png](output_27_1.png)



```python
(df1['Charitable_Ex']==0)
```




    0      False
    1      False
    2      False
    3       True
    4      False
           ...  
    342    False
    343    False
    344    False
    345     True
    346     True
    Name: Charitable_Ex, Length: 347, dtype: bool




```python
df1.shape
```




    (290, 21)




```python
# df1 = df1.loc[df1['Charitable_Ex']!=0, :]
```


```python
### SCIKIT-LEARN ###

cols = ['Govt_Funding']
X = df1[cols]
# y = np.log(df1['Charitable_Ex']+0.001)
y = df1['Charitable_Ex']

train_X, test_X, train_y, test_y = train_test_split(X,y,test_size=0.2, random_state=11)
regr_model = LinearRegression()
estimator = regr_model.fit(train_X, train_y) # Fitting model to training data and saving in variable.
prediction = estimator.predict(test_X)
regr_model.coef_
```




    array([1.15741847])




```python
# Evaluation using cross-validation:
# R-squared
print('Rsquared: {}\n'.format(metrics.explained_variance_score(test_y,prediction)))
# score the model on the train set
print('Train score: {}\n'.format(regr_model.score(train_X,train_y)))
# score the model on the test set
print('Test score: {}\n'.format(regr_model.score(test_X,test_y)))
# calculate the overall accuracy of the model
print('Overall model accuracy: {}\n'.format(r2_score(test_y,prediction)))
# compute the mean squared error of the model
print('Mean Squared Error: {}'.format(mean_squared_error(test_y,prediction)))
```

    Rsquared: 0.9407918273376905
    
    Train score: 0.8685287322651924
    
    Test score: 0.9395368525582409
    
    Overall model accuracy: 0.9395368525582409
    
    Mean Squared Error: 142352231227.08542
    


```python
df_pred=pd.DataFrame({'Actual':test_y, 'Predicted':prediction})
```


```python
ax = sns.lmplot(x='Predicted', y='Actual', data=df_pred, markers=['x'], fit_reg=False)
sns.lineplot([1,10e7], [1, 10e7], linewidth=2, color='darkgreen')
# ax.set(xlim=[10e1,10e7], ylim=[10e1,10e7])
```




    <matplotlib.axes._subplots.AxesSubplot at 0x1e405c39fc8>




![png](output_34_1.png)



```python
ax = sns.lmplot(x='Predicted', y='Actual', data=df_pred, markers=['x'], fit_reg=False)
sns.lineplot([1,10e7], [1, 10e7], linewidth=2, color='darkgreen')
ax.set(xscale='log', yscale='log', xlim=[10e1,10e7], ylim=[10e1,10e7])
```




    <seaborn.axisgrid.FacetGrid at 0x1e405d18588>




![png](output_35_1.png)


## conclusion:
Both models produced similar regression coefficients and high accuracy scores 
Howewer, the actual prediction power of the model is low with mean error around 280971. 
We will fit other regression models to check if it is possible to get better results

## Saeed:
The high absolute error is because of your high value number of the labels. In this situation normally we look at logoarithm of the values. And as you saw in the plots above, when you plot in log-scale the errors become understandable and the results look good.

## Ridge Regression


```python
ridge = Ridge(alpha=0)
ridge.fit(train_X,train_y)
ridge_pred = ridge.predict(test_X)
print('Train score: {}\n'.format(ridge.score(train_X,train_y)))
print('Test score: {}\n'.format(ridge.score(test_X,test_y)))
print('Overall model accuracy: {}\n'.format(r2_score(test_y,ridge_pred)))
print('Mean Squared Error: {}'.format(mean_squared_error(test_y,ridge_pred)))
```

    Train score: 0.9316884099672571
    
    Test score: 0.8242514891854011
    
    Overall model accuracy: 0.8242514891854011
    
    Mean Squared Error: 642267760366.4663
    

# Cross Validation


```python
# The same result, so we try to check the best parameters
from sklearn.model_selection import train_test_split,GridSearchCV
ridge_model = Ridge()
param = {'alpha':[0,0.1,0.01,0.001,1]}
ridge_search = GridSearchCV(ridge_model,param,cv=5,n_jobs=-1)
ridge_search.fit(train_X,train_y)
print('Best parameter found:\n{}'.format(ridge_search.best_params_))
print('Train score: {}\n'.format(ridge_search.score(train_X,train_y)))
print('Test score: {}'.format(ridge_search.score(test_X,test_y)))
```

    Best parameter found:
    {'alpha': 1}
    Train score: 0.07173098114011378
    
    Test score: 0.08720147132621547
    


```python
# we used the best one, and got the same prediction power
df_ridge=pd.DataFrame({'Actual':test_y, 'Predicted':ridge_pred})
df_ridge.head(10)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Actual</th>
      <th>Predicted</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>233</th>
      <td>57421</td>
      <td>79283.960759</td>
    </tr>
    <tr>
      <th>321</th>
      <td>308</td>
      <td>10866.271299</td>
    </tr>
    <tr>
      <th>328</th>
      <td>11303</td>
      <td>18872.917372</td>
    </tr>
    <tr>
      <th>303</th>
      <td>161293</td>
      <td>152406.653049</td>
    </tr>
    <tr>
      <th>265</th>
      <td>132779</td>
      <td>204837.568091</td>
    </tr>
    <tr>
      <th>75</th>
      <td>362778</td>
      <td>254474.318777</td>
    </tr>
    <tr>
      <th>22</th>
      <td>4305</td>
      <td>74506.171059</td>
    </tr>
    <tr>
      <th>206</th>
      <td>301930</td>
      <td>170200.577307</td>
    </tr>
    <tr>
      <th>93</th>
      <td>17065</td>
      <td>64037.085621</td>
    </tr>
    <tr>
      <th>157</th>
      <td>351833</td>
      <td>453634.243624</td>
    </tr>
  </tbody>
</table>
</div>




```python
ax = sns.lmplot(x='Predicted', y='Actual', data=df_pred, markers=['x'], fit_reg=False)
sns.lineplot([1,10e7], [1, 10e7], linewidth=2, color='darkgreen')
ax.set(xscale='log', yscale='log', xlim=[10e1,10e7], ylim=[10e1,10e7])
```




    <seaborn.axisgrid.FacetGrid at 0x1e47f6adcc8>




![png](output_42_1.png)


# Distribution Plots


```python
# the results are contradicting with metrics
#we will check the disrtribution of data and outliers
Chart_ex=df1['Charitable_Ex']
sns.distplot(Chart_ex, kde=False)  
fig = plt.gcf()
fig.set_size_inches(20, 6)
plt.show()
```


![png](output_44_0.png)



```python
Seniors=df1['Seniors']
sns.distplot(Seniors, kde=False)  
fig = plt.gcf()
fig.set_size_inches(20, 6)
plt.show()
```


![png](output_45_0.png)



```python
# Both are skewed to the right
plt.figure(figsize=(16,6))
ax = sns.boxplot('Year', 'Charitable_Ex', data=df1, width=0.3, palette="Set2")
ax.set(yscale='log')
```




    [None]




![png](output_46_1.png)


# The Results Are Good. Why bother with outliers?!! 
[ssuggestion: removing hereafter]

# Removing Outliers


```python
# we will drop the outliers (in Charitable_Ex) and fit the model again
df_clean=df1.drop(df1[(df1['Charitable_Ex'] > 1000000)].index)
df_clean.describe()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Year</th>
      <th>Population_total</th>
      <th>Seniors</th>
      <th>Donations</th>
      <th>Other_rev</th>
      <th>Govt_Funding</th>
      <th>Charitable_Ex</th>
      <th>Management_Admin</th>
      <th>Fund_Raising</th>
      <th>Political_Activities</th>
      <th>Gifts_to_others</th>
      <th>Other_ex</th>
      <th>Compensation</th>
      <th>Professional_Consulting</th>
      <th>Full_Time_Staff</th>
      <th>Part_time_Staff</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>308.000000</td>
      <td>308.000000</td>
      <td>308.000000</td>
      <td>3.080000e+02</td>
      <td>3.080000e+02</td>
      <td>3.080000e+02</td>
      <td>308.000000</td>
      <td>308.000000</td>
      <td>308.000000</td>
      <td>289.000000</td>
      <td>2.890000e+02</td>
      <td>3.060000e+02</td>
      <td>3.080000e+02</td>
      <td>308.000000</td>
      <td>308.000000</td>
      <td>308.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>2018.448052</td>
      <td>52821.685767</td>
      <td>7790.806818</td>
      <td>1.509613e+05</td>
      <td>1.499700e+05</td>
      <td>8.348538e+04</td>
      <td>128962.438312</td>
      <td>59216.551948</td>
      <td>27673.220779</td>
      <td>990.422145</td>
      <td>4.278829e+04</td>
      <td>4.216214e+04</td>
      <td>1.454295e+05</td>
      <td>8531.831169</td>
      <td>2.146104</td>
      <td>2.555195</td>
    </tr>
    <tr>
      <th>std</th>
      <td>1.529481</td>
      <td>97461.117930</td>
      <td>10452.233946</td>
      <td>4.718865e+05</td>
      <td>5.194542e+05</td>
      <td>2.928006e+05</td>
      <td>180059.155222</td>
      <td>130883.274322</td>
      <td>84032.300636</td>
      <td>16837.176471</td>
      <td>3.451917e+05</td>
      <td>3.918162e+05</td>
      <td>4.034986e+05</td>
      <td>18720.606085</td>
      <td>3.867692</td>
      <td>4.089417</td>
    </tr>
    <tr>
      <th>min</th>
      <td>2015.000000</td>
      <td>313.000000</td>
      <td>39.310551</td>
      <td>0.000000e+00</td>
      <td>0.000000e+00</td>
      <td>0.000000e+00</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000e+00</td>
      <td>0.000000e+00</td>
      <td>0.000000e+00</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>2017.000000</td>
      <td>5278.610884</td>
      <td>1119.182824</td>
      <td>8.834250e+03</td>
      <td>9.650000e+02</td>
      <td>0.000000e+00</td>
      <td>5223.250000</td>
      <td>419.750000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000e+00</td>
      <td>0.000000e+00</td>
      <td>0.000000e+00</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>2018.000000</td>
      <td>14980.087406</td>
      <td>2825.500000</td>
      <td>3.474150e+04</td>
      <td>1.298950e+04</td>
      <td>2.325500e+04</td>
      <td>46066.500000</td>
      <td>9412.500000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000e+00</td>
      <td>0.000000e+00</td>
      <td>3.340250e+04</td>
      <td>2829.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>2020.000000</td>
      <td>45120.500000</td>
      <td>10129.750000</td>
      <td>1.466022e+05</td>
      <td>8.865975e+04</td>
      <td>9.558325e+04</td>
      <td>180027.750000</td>
      <td>46992.750000</td>
      <td>12809.750000</td>
      <td>0.000000</td>
      <td>0.000000e+00</td>
      <td>0.000000e+00</td>
      <td>1.983688e+05</td>
      <td>9911.000000</td>
      <td>3.000000</td>
      <td>3.000000</td>
    </tr>
    <tr>
      <th>max</th>
      <td>2021.000000</td>
      <td>519883.000000</td>
      <td>48293.000000</td>
      <td>7.319872e+06</td>
      <td>7.074869e+06</td>
      <td>4.821174e+06</td>
      <td>906976.000000</td>
      <td>988425.000000</td>
      <td>672699.000000</td>
      <td>286232.000000</td>
      <td>4.654819e+06</td>
      <td>6.581173e+06</td>
      <td>6.396932e+06</td>
      <td>218508.000000</td>
      <td>34.000000</td>
      <td>33.000000</td>
    </tr>
  </tbody>
</table>
</div>




```python
lm_mult_clean = smf.ols(formula='Charitable_Ex ~ Govt_Funding + Seniors', data=df_clean).fit()
lm_mult_clean.rsquared
```




    0.38164249357010993




```python
cols = ['Govt_Funding','Seniors']
X1 = df_clean[cols]
y1 = df_clean['Charitable_Ex']
train_X1, test_X1, train_y1, test_y1 = train_test_split(X,y,test_size=0.2)
regr_model_clean = LinearRegression()
estimator_clean = regr_model_clean.fit(train_X1, train_y1) # Fitting model to training data and saving in variable.
prediction_clean = estimator_clean.predict(test_X1)
print('Rsquared: {}\n'.format(metrics.explained_variance_score(test_y1,prediction_clean)))
# score the model on the train set
print('Train score: {}\n'.format(regr_model.score(train_X1,train_y1)))
# score the model on the test set
print('Test score: {}\n'.format(regr_model.score(test_X1,test_y1)))
# calculate the overall accuracy of the model
print('Overall model accuracy: {}\n'.format(r2_score(test_y1,prediction_clean)))
# compute the mean squared error of the model
print('Mean Squared Error: {}'.format(mean_squared_error(test_y1,prediction_clean)))
```

    Rsquared: 0.639763847837096
    
    Train score: 0.903501194647658
    
    Test score: 0.6418313284301984
    
    Overall model accuracy: 0.6366958659474109
    
    Mean Squared Error: 163865298038.3273
    


```python
# Overall, we again got poor results (overfitting model and high mean error). Eliminating outliers didn't help with high variability
# Next, we will try to normalize distribution of the variables
from sklearn import preprocessing
X_scaled = preprocessing.scale(X1)
X_scaled = pd.DataFrame(X_scaled,columns=X1.columns)
X_train,X_test,y_train,y_test = train_test_split(X_scaled,y1,test_size=.2,random_state=0)
model = LinearRegression()
model.fit(X_train,y_train)
linear_pred = model.predict(X_test)
print('Train score: {}\n'.format(model.score(X_train,y_train)))
# score the model on the test set
print('Test score: {}\n'.format(model.score(X_test,y_test)))
# calculate the overall accuracy of the model
print('Overall model accuracy: {}\n'.format(r2_score(y_test,linear_pred)))
# compute the mean squared error of the model
print('Mean Squared Error: {}'.format(mean_squared_error(y_test,linear_pred)))
```

    Train score: 0.3279135159143608
    
    Test score: 0.48288834395983526
    
    Overall model accuracy: 0.48288834395983526
    
    Mean Squared Error: 24473792209.034832
    


```python
# Now, metrics show underfitting model with low accuracy and mean error 156441.018
# Conclusion for the dataset with all observations is that two variables are not enough for effective prediction.
```

Multiple Regression Models and Evaluation (dataset with Community Hospice Type observations)


```python
### STATSMODELS ###

# include Gov_Funding and Seniors in the model
lm_mult_com = smf.ols(formula='Charitable_Ex ~ Govt_Funding + Donations + Seniors', data=df_com).fit()
lm_mult_com.rsquared
```




    0.5881847125947737




```python
### SCIKIT-LEARN ###
cols_com = ['Govt_Funding','Donations','Seniors']
X2 = df_com[cols_com]
y2 = df_com['Charitable_Ex']
train_X2, test_X2, train_y2, test_y2 = train_test_split(X2,y2,test_size=0.2)
regr_model_com = LinearRegression()
estimator1 = regr_model_com.fit(train_X2, train_y2) # Fitting model to training data and saving in variable.
prediction1 = estimator1.predict(test_X2)
```


```python
# R-squared
print('Rsquared: {}\n'.format(metrics.explained_variance_score(test_y2,prediction1)))
# score the model on the train set
print('Train score: {}\n'.format(regr_model_com.score(train_X2,train_y2)))
# score the model on the test set
print('Test score: {}\n'.format(regr_model_com.score(test_X2,test_y2)))
# calculate the overall accuracy of the model
print('Overall model accuracy: {}\n'.format(r2_score(test_y2,prediction1)))
# compute the mean squared error of the model
print('Mean Squared Error: {}'.format(mean_squared_error(test_y2,prediction1)))
```

    Rsquared: 0.4261314963689121
    
    Train score: 0.6078981348394306
    
    Test score: 0.4243714822893603
    
    Overall model accuracy: 0.4243714822893603
    
    Mean Squared Error: 13521693015.864574
    


```python
# evaluating metrics show overfitting model and low accuracy.
# So, we will come back to our first univariate regression this time using seniors dinamic to predict charitable expences.
```

Univariate Regression Models and Evaluation (target-charitable expences depanding on seniors)


```python
sns.pairplot(df1, x_vars=['Seniors'], y_vars='Charitable_Ex', height=7, aspect=0.7,diag_kind=None, kind='reg')
```




    <seaborn.axisgrid.PairGrid at 0x23331c2f6d0>




![png](output_60_1.png)



```python
lm_univar = smf.ols(formula='Charitable_Ex ~ Seniors', data=df1).fit()
lm_univar.params
```




    Intercept   -100912.480584
    Seniors          60.035104
    dtype: float64




```python
lm_univar.pvalues
```




    Intercept    2.120018e-01
    Seniors      3.001967e-27
    dtype: float64




```python
lm_univar.rsquared
```




    0.28776752535823125




```python
sns.pairplot(df_com, x_vars=['Seniors'], y_vars='Charitable_Ex', height=7, aspect=0.7,diag_kind=None, kind='reg')
```




    <seaborn.axisgrid.PairGrid at 0x23331fdb430>




![png](output_64_1.png)



```python
lm_univar1 = smf.ols(formula='Charitable_Ex ~ Seniors', data=df_com).fit()
lm_univar1.rsquared
```




    0.49231958919229424




```python
lm_univar.params
```




    Intercept   -100912.480584
    Seniors          60.035104
    dtype: float64



 For both datasets (with all observations and with community hospices only) we got the same coefficients
 In the whole dataset they explain only 29% of expences values while in community dataset 50 % meaning the less degree of variability in the latter one.
 
 All in all, there are several assumptions:
 1. dataset is inadequate
 2. seniors dinamic only cannot explain and help to predict the expences.


```python

```
